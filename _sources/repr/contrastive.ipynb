{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2b3f6b",
   "metadata": {},
   "source": [
    "# Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42531c8b",
   "metadata": {},
   "source": [
    "Contrastive learning is a self-supervised learning method that learns feature representations to distinguish between similar and different samples.\n",
    "\n",
    "![image](https://lh4.googleusercontent.com/NFYj5oT4MCnqcMSu_asvwqAb1A8rmzZr0jB2SZjMJiOGEz_my3VWcZq_jOGMogc_fwnuFGmXbaHjAa6eugaeauio1uvgR5W9be7tn_Nxo0jXiWGIb-eB9x6km22ZM6kOqIA0YAyr)\n",
    "\n",
    "Source: [https://analyticsindiamag.com/contrastive-learning-self-supervised-ml/](https://analyticsindiamag.com/contrastive-learning-self-supervised-ml/)\n",
    "\n",
    "\n",
    "Contrastive learning can be formulated as a score:\n",
    "\n",
    "$$ score(f(x),f(x^+))>score(f(x),f(x^-)) $$\n",
    "\n",
    "where $x^+$ and $x^-$ refers to a sample similar and different to $x$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e3a615",
   "metadata": {},
   "source": [
    "## What is SimCLR?\n",
    "\n",
    "[SimCLR](https://arxiv.org/abs/2002.05709) is a contrastive learning framework introduced by Chen et. al that outperforms the previously SOTA supervised learning method on ImageNet when scaling up the architecture.\n",
    "\n",
    "![https://amitness.com/images/simclr-general-architecture.png](https://amitness.com/images/simclr-general-architecture.png)\n",
    "\n",
    "Source: [https://amitness.com/2020/03/illustrated-simclr/](https://amitness.com/2020/03/illustrated-simclr/)\n",
    "\n",
    "Essentially, SimCLR applies data augmentation on a single image to obtain 2 augmented images, and then optimises by maximising the similarity between the representations of both augmented images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e17b3",
   "metadata": {},
   "source": [
    "## How is similarity computed?\n",
    "\n",
    "SimCLR applies cosine similarity between the embeddings of images, $z_i, z_j$:\n",
    "\n",
    "$$ s_{i,j}=\\frac{z_i^Tz_j}{(\\tau||z_i||||z_j||)} $$\n",
    "\n",
    "where \\tau is the temperature that scales the range of cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d497d3",
   "metadata": {},
   "source": [
    "## What loss does SimCLR optimise?\n",
    "\n",
    "SimCLR uses NT-Xent loss (Normalized Temperature-scaled Cross-entropy loss).\n",
    "\n",
    "First, obtain the Noise Contrastive Estimation (NCE) loss:\n",
    "\n",
    "$$ l(i,j)=-log\\frac{exp(s_{i,j})}{\\sum_{k=1}^{2N}l_{[k!=i]}exp(s_{i,k})} $$\n",
    "\n",
    "Then compute average loss over all pairs in the batch of size N=2:\n",
    "\n",
    "$$ L=\\frac{1}{2N}\\sum_{k=1}^N[l(2k-1,2k)+l(2k,2k-1)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfd610",
   "metadata": {},
   "source": [
    "## So what?\n",
    "\n",
    "The encoder representations of the image (not the projection head representations) can be used for downstream tasks such as ImageNet classification.\n",
    "\n",
    "![https://amitness.com/images/simclr-performance.png](https://amitness.com/images/simclr-performance.png)\n",
    "\n",
    "Source: [https://arxiv.org/abs/2002.05709](https://arxiv.org/abs/2002.05709)\n",
    "\n",
    "It is shown that on ImageNet ILSVRC-2012, it achieves 76.5% top-1 accuracy, a 7% improvement over the previous SOTA self-supervised method, [Contrastive Predictive Coding](https://arxiv.org/abs/1905.09272), and comparable with supervised method ResNet50."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (book)",
   "language": "python",
   "name": "book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
